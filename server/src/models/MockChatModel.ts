import { AIModel } from './types.js';

export class MockChatModel implements AIModel {
  async invoke(messages: any[]) {
    const lastMessage = messages[messages.length - 1];
    const userMessage = lastMessage.content || 'Hello';
    
    const responses = [
      `This is a mock response to: "${userMessage}". In a real setup, this would be generated by an AI model.`,
      `Mock AI: I understand you said "${userMessage}". This is a test response since the actual AI model is not available.`,
      `Response to "${userMessage}": This is a placeholder response. Please install Ollama or configure OpenAI API key for real AI responses.`,
      `Mock response: "${userMessage}" - This demonstrates the chat interface works even without a real AI model.`
    ];
    
    const randomResponse = responses[Math.floor(Math.random() * responses.length)];
    
    return {
      content: randomResponse,
      response_metadata: {
        usage: {
          prompt_tokens: userMessage.length,
          completion_tokens: randomResponse.length,
          total_tokens: userMessage.length + randomResponse.length
        }
      }
    };
  }
  
  async *stream(messages: any[]) {
    const lastMessage = messages[messages.length - 1];
    const userMessage = lastMessage.content || 'Hello';
    
    const response = `Mock streaming response to: "${userMessage}". This simulates real-time AI generation.`;
    
    for (const char of response) {
      yield { content: char };
      // Simulate typing delay
      await new Promise(resolve => setTimeout(resolve, 50));
    }
  }
  
  pipe(parser: any) {
    return {
      invoke: async (messages: any[]) => {
        const result = await this.invoke(messages);
        return result.content;
      }
    };
  }
} 